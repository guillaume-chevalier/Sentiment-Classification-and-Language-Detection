{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_Bk\r\n",
      "pos_Bk\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 ./data/task1/Book/\n",
    "# out: \n",
    "# neg_Bk\n",
    "# pos_Bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "neg_Bk_files = glob.glob(os.path.join(\".\", \"data\", \"task1\", \"Book\", \"neg_Bk\", \"*.text\"))\n",
    "pos_Bk_files = glob.glob(os.path.join(\".\", \"data\", \"task1\", \"Book\", \"pos_Bk\", \"*.text\"))\n",
    "\n",
    "\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding=\"ISO-8859-1\") as fml:\n",
    "        crushed_content = fml.read().rstrip(\"\\n\").strip()\n",
    "    return crushed_content\n",
    "\n",
    "def load_all_files(file_paths):\n",
    "    return [load_file(path) for path in file_paths]\n",
    "\n",
    "def load_all_data_task1(pos_files_paths, neg_files_paths):\n",
    "\n",
    "    pos = load_all_files(pos_files_paths)\n",
    "    neg = load_all_files(neg_files_paths)\n",
    "\n",
    "    X = pos + neg\n",
    "    y = [1] * len(pos) + [0] * len(neg)\n",
    "    \n",
    "    # Have non-concentrated classes, but with seed for reproducibility:\n",
    "    X, y = sklearn.utils.shuffle(X, y, random_state=42)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = load_all_data_task1(neg_Bk_files, pos_Bk_files)\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return [word_tokenize(sentence) for sentence in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToLowerCase(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return [[word.lower() for word in sentence] for sentence in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "class RemoveStopWords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, **k): \n",
    "        self.stop_words = [\n",
    "            sw.lower() for sw in (\n",
    "                set(sklearn.feature_extraction.stop_words.ENGLISH_STOP_WORDS) &\n",
    "                set(stopwords.words('english'))\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return [[word for word in sentence if word.lower() not in self.stop_words] for sentence in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepOpenClassesOnly(BaseEstimator, TransformerMixin): \n",
    "    \"\"\"\n",
    "    Categories that will usually be open classes:\n",
    "    - adjectives\n",
    "    - adverbs\n",
    "    - nouns\n",
    "    - verbs (except auxiliary verbs)\n",
    "    - interjections\n",
    "    \n",
    "    Keep Open Classes or things that contains at least some of Open Classes:\n",
    "    - adjectives:\n",
    "    JJ: adjective or numeral, ordinal\n",
    "    JJR: adjective, comparative\n",
    "    JJS: adjective, superlative\n",
    "    - adverbs:\n",
    "    RB: adverb\n",
    "    RBR: adverb, comparative\n",
    "    RBS: adverb, superlative\n",
    "    WRB: Wh-adverb\n",
    "    - nouns:\n",
    "    NN: noun, common, singular or mass\n",
    "    NNP: noun, proper, singular\n",
    "    NNPS: noun, proper, plural\n",
    "    NNS: noun, common, plural\n",
    "    - verbs (except auxiliary verbs):\n",
    "    VB: verb, base form\n",
    "    VBD: verb, past tense\n",
    "    VBG: verb, present participle or gerund\n",
    "    VBN: verb, past participle\n",
    "    VBP: verb, present tense, not 3rd person singular\n",
    "    VBZ: verb, present tense, 3rd person singular\n",
    "    - interjections:\n",
    "    UH: interjection\n",
    "\n",
    "    Other words that are not in \"Open Classes\": \n",
    "    CC: conjunction, coordinating\n",
    "    CD: numeral, cardinal\n",
    "    DT: determiner\n",
    "    EX: existential there\n",
    "    FW: foreign word\n",
    "    IN: preposition or conjunction, subordinating\n",
    "    LS: list item marker\n",
    "    MD: modal auxiliary\n",
    "    PDT: pre-determiner\n",
    "    POS: genitive marker\n",
    "    PRP: pronoun, personal\n",
    "    PRP$: pronoun, possessive\n",
    "    RP: particle\n",
    "    SYM: symbol\n",
    "    TO: \"to\" as preposition or infinitive marker\n",
    "    WDT: WH-determiner\n",
    "    WP: WH-pronoun\n",
    "    WP$: WH-pronoun, possessive\n",
    "\n",
    "    Other that are kept, because they are not \"words\",\n",
    "    but rather misc things we might want to keep: \n",
    "    $: dollar\n",
    "    '': closing quotation mark\n",
    "    ``: opening quotation mark\n",
    "    (: opening parenthesis\n",
    "    ): closing parenthesis\n",
    "    ,: comma\n",
    "    --: dash\n",
    "    .: sentence terminator\n",
    "    :: colon or ellipsis\n",
    "\n",
    "    References: \n",
    "    - https://en.wikipedia.org/wiki/Part_of_speech#Functional_classification\n",
    "    - https://www.nltk.org/book/ch05.html\n",
    "    \"\"\"\n",
    "    WORDS_OPEN_CLASSES = [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"WRB\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"UH\"]\n",
    "    WORDS_CLOSED_CLASSES_OR_OTHER_MISC = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"LS\", \"MD\", \"PDT\", \"POS\", \"PRP\", \"PRP\", \"RP\", \"SYM\", \"TO\", \"WDT\", \"WP\", \"WP\"]\n",
    "    OTHER_NOT_WORDS = [\"$\", \"''\", \"``\", \"(\", \")\", \",\", \"--\", \".\", \":\"]\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        self.all_tags = set()\n",
    "        # TODO: do.\n",
    "        return x\n",
    "\n",
    "    def _tag_all_filter(self, X):\n",
    "        filtered = [_tag_filter(x) for x in X[:10]]\n",
    "        return filtered\n",
    "\n",
    "    def _tag_filter(self, x): \n",
    "        tagged = nltk.pos_tag(word_tokenize(x))\n",
    "        \n",
    "        for (word, tag) in tagged: # TODO: del this.\n",
    "            self.all_tags.add(tag)\n",
    "        \n",
    "        WORDS_TO_KEEP = OpenClassesOnly.WORDS_OPEN_CLASSES + OpenClassesOnly.OTHER_NOT_WORDS\n",
    "        \n",
    "        return [word for (word, tag) in tagged if tag in WORDS_TO_KEEP]\n",
    "    \n",
    "    def print_more_info(self):\n",
    "        \"\"\"\n",
    "        Print the definition of all NLTK's tags:\n",
    "        \"\"\"\n",
    "        nltk.download('tagsets')\n",
    "        nltk.help.upenn_tagset()\n",
    "        print(\"WORDS_OPEN_CLASSES: \", WORDS_OPEN_CLASSES)\n",
    "        print(\"WORDS_CLOSED_CLASSES_OR_OTHER_MISC: \", WORDS_CLOSED_CLASSES_OR_OTHER_MISC)\n",
    "        print(\"OTHER_NOT_WORDS: \", OTHER_NOT_WORDS)\n",
    "        print(\"WORDS_TO_KEEP = WORDS_OPEN_CLASSES + OTHER_NOT_WORDS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedPorterStemmer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "        transformed_x = []\n",
    "        for tokenized_sample in x: \n",
    "            transformed_sample = []\n",
    "            for word in tokenized_sample: \n",
    "                if type(word) != str:\n",
    "                    print(type(word), len(word))\n",
    "                transformed_sample.append(\n",
    "                    stemmer.stem(word)\n",
    "                )\n",
    "            transformed_x.append(transformed_sample)\n",
    "                \n",
    "        return transformed_x\n",
    "\n",
    "# s = \"bacon - I like that, ouch wow damn hey bye okay oh m-hm huh\"\n",
    "# WrappedPorterStemmer().fit_transform(X=NLTKTokenizer().fit_transform([s])[0])\n",
    "\"\"\"\n",
    "stemmer = PorterStemmer()\n",
    "new_text = word_tokenize(s)\n",
    "for w in new_text:\n",
    "    print(stemmer.stem(w))\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapePrinter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, name, **params):\n",
    "        self.name = name\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        print(\"FIT:\", self.name, x.shape)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        print(\"TRANSFORM:\", self.name, x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters (Cross Validation macro accuracy score=0.745):\n",
      "{'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': False, 'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': 0.98, 'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': 10000, 'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': 2, 'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': (1, 2), 'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': None, 'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': None, 'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': <function <lambda> at 0x7fa5820e12f0>, 'logistic_regression__C': 100000.0, 'logistic_regression__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('nltk_tokenizer', NLTKTokenizer()),\n",
    "    ('to_lower_case', ToLowerCase()),\n",
    "    ('remove_stop_words', RemoveStopWords()),\n",
    "    ('keep_open_classes_only', KeepOpenClassesOnly()),\n",
    "    ('porter_stemmer', WrappedPorterStemmer()),\n",
    "    ('count_vect_that_remove_unfrequent_words_and_stopwords', CountVectorizer()),\n",
    "    ('logistic_regression', LogisticRegression()),\n",
    "])\n",
    "\n",
    "hyperparams_grid = {\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': [0.98],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': [2],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': [10000],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': [None],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': [lambda x: x],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': [None],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': [False],\n",
    "    'logistic_regression__C': [1e4, 1e5],\n",
    "    'logistic_regression__solver': ['lbfgs']\n",
    "    # 'logistic_regression__alpha': np.logspace(-4, 4, 5),\n",
    "    # 'logistic_regression__multi_class': 2\n",
    "}\n",
    "_ = \"\"\"\n",
    "hyperparams_grid = {\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': [0.98],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': [2],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': [10000],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': [(1, 1)],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': [None],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': [lambda x: x],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': [None],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': [False],\n",
    "    'logistic_regression__C': [1e4],\n",
    "    'logistic_regression__solver': ['lbfgs']\n",
    "    # 'logistic_regression__alpha': np.logspace(-4, 4, 5),\n",
    "    # 'logistic_regression__multi_class': 2\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, hyperparams_grid, iid=False, cv=3, return_train_score=False, verbose=1, scoring=\"accuracy\")\n",
    "# TODO: increase CV to 5 such as:\n",
    "# grid_search = GridSearchCV(pipeline, hyperparams_grid, iid=False, cv=5, return_train_score=False, verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best hyperparameters (Cross Validation macro accuracy score=%0.3f):\" % grid_search.best_score_)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7375 0.78   0.7525 0.7375 0.7525]\n"
     ]
    }
   ],
   "source": [
    "# Retrain best model: \n",
    "\n",
    "best_pipeline = Pipeline([\n",
    "    ('nltk_tokenizer', NLTKTokenizer()),\n",
    "    ('to_lower_case', ToLowerCase()),\n",
    "    ('remove_stop_words', RemoveStopWords()),\n",
    "    ('keep_open_classes_only', KeepOpenClassesOnly()),\n",
    "    ('porter_stemmer', WrappedPorterStemmer()),\n",
    "    ('count_vect_that_remove_unfrequent_words_and_stopwords', CountVectorizer()),\n",
    "    # ('shapr', ShapePrinter(\"shapr\")),\n",
    "    ('logistic_regression', LogisticRegression()),\n",
    "])\n",
    "best_pipeline.set_params(\n",
    "    **best_params\n",
    ")\n",
    "# best_pipeline.fit(X, y)\n",
    "# print(((best_pipeline.predict(X) == y)*1.0).mean())\n",
    "scores = cross_val_score(best_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.735 , 0.71  , 0.7375, 0.7175, 0.6975])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_params = {\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': 0.98,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': 2,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': 10000,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': (1, 2),\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': None,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': lambda x: x,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': None,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': False,\n",
    "    #'tsvd__n_components': 1024,\n",
    "    \"xgb__max_depth\": 20,\n",
    "    \"xgb__n_estimators\": 20,\n",
    "    \"xgb__learning_rate\": 1\n",
    "}\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('nltk_tokenizer', NLTKTokenizer()),\n",
    "    ('to_lower_case', ToLowerCase()),\n",
    "    ('remove_stop_words', RemoveStopWords()),\n",
    "    ('keep_open_classes_only', KeepOpenClassesOnly()),\n",
    "    ('porter_stemmer', WrappedPorterStemmer()),\n",
    "    ('count_vect_that_remove_unfrequent_words_and_stopwords', CountVectorizer()),\n",
    "    # ('shapr', ShapePrinter(\"shapr\")),\n",
    "    #('tsvd', TruncatedSVD()),\n",
    "    ('xgb', xgb.XGBClassifier()),\n",
    "])\n",
    "xgb_pipeline.set_params(\n",
    "    **new_params\n",
    ")\n",
    "# xgb_pipeline.fit(X, y)\n",
    "# print(((xgb_pipeline.predict(X) == y)*1.0).mean())\n",
    "scores = cross_val_score(xgb_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection d’attributs: Tous les attributs, avec classes ouvertes seulement, sans les mots outils:\n",
    "# Ne garder que les mots appartenant à des classes ouvertes (c.-à-d. les noms,\n",
    "# adjectifs, verbes et adverbes). Vous devez faire une analyse grammaticale (POS \n",
    "# tagging) des textes pour identifier ces mots. \n",
    "\n",
    "# Stemming. Optionnel: aussi WordNetLemmatizer de NLTK.\n",
    "\n",
    "# Valeurs d’attributs: Compte de mots: TF. Optionnel:  présence et tf-idf\n",
    "\n",
    "# Autres attributs: Nombre de mots positifs/négatifs \n",
    "# (aka Le nombre de mots dont la polarité est positive ou négative. Vous pouvez utiliser SentiWordnet (NLTK) ou un autre lexique pour estimer cet attribut.)\n",
    "\n",
    "# Naive bayes, régression logistique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'logistic__alpha': np.logspace(-4, 4, 9),\n",
    "}\n",
    "np.logspace(-4, 4, 9).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
