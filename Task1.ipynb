{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_Bk\r\n",
      "pos_Bk\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 ./data/task1/Book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to /home/gui/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Python 3.6\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import xgboost as xgb\n",
    "\n",
    "from src.pipeline_steps.nltk_word_tokenize import NLTKTokenizer\n",
    "from src.pipeline_steps.to_lower_case import ToLowerCase\n",
    "from src.pipeline_steps.remove_stop_words import RemoveStopWords\n",
    "from src.pipeline_steps.keep_open_classes_only import KeepOpenClassesOnly\n",
    "from src.pipeline_steps.sentiwordnet import SentiWordNetPosNegAttributes\n",
    "from src.pipeline_steps.porter_stemmer import PorterStemmerStep\n",
    "from src.pipeline_steps.data_shape_printer import ShapePrinter\n",
    "from src.data_loading.task_1 import load_all_data_task_1\n",
    "from src.text_classifier_pipelines.stop_words_open_class_stemmer.pipeline_factory import find_and_train_best_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 1600 400 400\n"
     ]
    }
   ],
   "source": [
    "neg_Bk_files = glob.glob(os.path.join(\".\", \"data\", \"task1\", \"Book\", \"neg_Bk\", \"*.text\"))\n",
    "pos_Bk_files = glob.glob(os.path.join(\".\", \"data\", \"task1\", \"Book\", \"pos_Bk\", \"*.text\"))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_all_data_task_1(neg_Bk_files, pos_Bk_files)\n",
    "\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will start Cross Validation for Logistic Classifiers.\n",
      "\n",
      "Cross-Validation Grid Search for: 'NAMEE TODO'...\n",
      "Best hyperparameters for 'NAMEE TODO' (3-folds cross validation accuracy score=0.694375):\n",
      "{'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': False, 'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': 0.98, 'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': 50000, 'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': 2, 'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': (1, 1), 'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': None, 'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': None, 'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': <function get_generic_hyperparams_grid.<locals>.<lambda> at 0x7ff104800840>, 'logistic_regression__C': 10000.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_trained_pipelines = find_and_train_best_pipelines(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final test: classifying on test documents of full-length:\n",
      "\n",
      "Test set score for '1-gram Char Logistic Classifier': 70.0%\n",
      "Test set was of 20% of full data, which was held-out of cross validation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The final test: classifying on test documents of full-length:\")\n",
    "print(\"\")\n",
    "for (model_name, model) in best_trained_pipelines.items():\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"Test set score for '{}': {}%\".format(model_name, score*100))\n",
    "    print(\"Test set was of 20% of full data, which was held-out of cross validation.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters (Cross Validation macro accuracy score=0.777):\n",
      "{'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': False, 'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': 0.98, 'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': 50000, 'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': 2, 'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': (1, 3), 'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': None, 'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': None, 'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': <function <lambda> at 0x7f9b8c13bbf8>, 'logistic_regression__C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('nltk_tokenizer', NLTKTokenizer()),\n",
    "    ('to_lower_case', ToLowerCase()),\n",
    "    ('remove_stop_words', RemoveStopWords()),\n",
    "    ('keep_open_classes_only', KeepOpenClassesOnly()),\n",
    "    ('sentiwordnet_attribute_pos_neg_count', SentiWordNetPosNegAttributes()),\n",
    "    ('porter_stemmer', PorterStemmerStep()),\n",
    "    ('count_vect_that_remove_unfrequent_words_and_stopwords', CountVectorizer()),\n",
    "    ('logistic_regression', LogisticRegression()),\n",
    "])\n",
    "\n",
    "hyperparams_grid = {\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': [0.98],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': [2],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': [50000],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': [None],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': [lambda x: x],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': [None],\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': [False],\n",
    "    'logistic_regression__C': [1e-2, 1.0, 1e2, 1e4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, hyperparams_grid, iid=True, cv=3, return_train_score=False, verbose=1, scoring=\"accuracy\")\n",
    "# TODO: increase CV to 5 such as:\n",
    "# grid_search = GridSearchCV(pipeline, hyperparams_grid, iid=False, cv=5, return_train_score=False, verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best hyperparameters (Cross Validation macro accuracy score=%0.3f):\" % grid_search.best_score_)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8025 0.8075 0.7875 0.7875 0.7625]\n"
     ]
    }
   ],
   "source": [
    "# Retrain best model: \n",
    "\n",
    "best_pipeline = Pipeline([\n",
    "    ('nltk_tokenizer', NLTKTokenizer()),\n",
    "    ('to_lower_case', ToLowerCase()),\n",
    "    ('remove_stop_words', RemoveStopWords()),\n",
    "    ('keep_open_classes_only', KeepOpenClassesOnly()),\n",
    "    ('sentiwordnet_attribute_pos_neg_count', SentiWordNetPosNegAttributes()),\n",
    "    ('porter_stemmer', PorterStemmerStep()),\n",
    "    ('count_vect_that_remove_unfrequent_words_and_stopwords', CountVectorizer()),\n",
    "    # ('shapr', ShapePrinter(\"shapr\")),\n",
    "    ('logistic_regression', LogisticRegression()),\n",
    "])\n",
    "best_pipeline.set_params(\n",
    "    **best_params\n",
    ")\n",
    "scores = cross_val_score(best_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline.fit(X, y)\n",
    "best_pipeline.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995\n"
     ]
    }
   ],
   "source": [
    "print(((best_pipeline.predict(X) == y)*1.0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.735 , 0.71  , 0.7375, 0.7175, 0.6975])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_params = {\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_df': 0.98,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__min_df': 2,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__max_features': 50000,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__ngram_range': (1, 2),\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__strip_accents': None,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__tokenizer': lambda x: x,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__preprocessor': None,\n",
    "    'count_vect_that_remove_unfrequent_words_and_stopwords__lowercase': False,\n",
    "    #'tsvd__n_components': 1024,\n",
    "    \"xgb__max_depth\": 20,\n",
    "    \"xgb__n_estimators\": 20,\n",
    "    \"xgb__learning_rate\": 1\n",
    "}\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('nltk_tokenizer', NLTKTokenizer()),\n",
    "    ('to_lower_case', ToLowerCase()),\n",
    "    ('remove_stop_words', RemoveStopWords()),\n",
    "    ('keep_open_classes_only', KeepOpenClassesOnly()),\n",
    "    ('porter_stemmer', PorterStemmerStep()),\n",
    "    ('count_vect_that_remove_unfrequent_words_and_stopwords', CountVectorizer()),\n",
    "    # ('shapr', ShapePrinter(\"shapr\")),\n",
    "    #('tsvd', TruncatedSVD()),\n",
    "    ('xgb', xgb.XGBClassifier()),\n",
    "])\n",
    "xgb_pipeline.set_params(\n",
    "    **new_params\n",
    ")\n",
    "# xgb_pipeline.fit(X, y)\n",
    "# print(((xgb_pipeline.predict(X) == y)*1.0).mean())\n",
    "scores = cross_val_score(xgb_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection d’attributs: Tous les attributs, avec classes ouvertes seulement, sans les mots outils:\n",
    "# Ne garder que les mots appartenant à des classes ouvertes (c.-à-d. les noms,\n",
    "# adjectifs, verbes et adverbes). Vous devez faire une analyse grammaticale (POS \n",
    "# tagging) des textes pour identifier ces mots. \n",
    "\n",
    "# Stemming. Optionnel: aussi WordNetLemmatizer de NLTK.\n",
    "\n",
    "# Valeurs d’attributs: Compte de mots: TF. Optionnel:  présence et tf-idf\n",
    "\n",
    "# Autres attributs: Nombre de mots positifs/négatifs \n",
    "# (aka Le nombre de mots dont la polarité est positive ou négative. Vous pouvez utiliser SentiWordnet (NLTK) ou un autre lexique pour estimer cet attribut.)\n",
    "\n",
    "# Naive bayes, régression logistique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'logistic__alpha': np.logspace(-4, 4, 9),\n",
    "}\n",
    "np.logspace(-4, 4, 9).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-01221b50913d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbreakdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbreakdown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/sentiwordnet.py\u001b[0m in \u001b[0;36msenti_synset\u001b[0;34m(self, *vals)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSentiSynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0msynset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m's'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;31m# split name into lemma, part of speech and synset number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_index_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m         \u001b[0msynset_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset_index_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "breakdown = swn.senti_synset('the')\n",
    "print(breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swnsentinegative', 'swnsentinegative', 'swnsentipositive', 'joy', 'not', 'anger']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "document = [\"joy\", \"not\", \"anger\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = swn.all_senti_synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = swn.senti_synsets('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
